
\section{Euchre Agents}

The overall objective in euchre is to take as many tricks as possible. This ensures the other team does not benefit from them.
Generally, this strategy works well, but it ignores the team aspect of the game. In this section, we give several euchre strategies
making various use of the information available in the game. All agents know the basic rules of the game, as a useful agent could
not be constructed without this knowledge. Additionally, no agent can cheat. When we say an agent chooses to play a card with some
property, it is assumed that the card is chosen from the set of legally playable cards. These strategies range from simple to selfish to
complex. These strategies aimed to produce a very quick but good decision when playing a card from their hand into a trick.


\subsection{Simple Strategies}

The first strategies discussed will be very simple, but serve as a very good base for more complex strategies. These agents only make use
of the information given directly to them, their hand. The first agent plays a random card from their hand. This agent exists to serve
as a basis for comparison. The next two agents are opposites of each other, one always plays the lowest card in their hand while
the other always plays the highest card. Since these agents do not use any additional information, they all play selfishly.

The agents High and Low serve as an excellent foundation for more complex agents. In euchre, the overall goal is to maximize the number
of tricks your team earns. Playing High will give you the best possible chance of winning a trick, while Low allows you to throw away
your worst card if you resign a trick. Playing a ``Middle'' strategy would not help compared to High and Low as it seems to give you
the worst traits of both the other strategies. With Middle, you could have an increased chance of taking a trick, but if you lost the trick,
you've lost a decent card which could have been used later to win a different trick. Do to this, no Middle agent was explored.


\subsection{More Complex Strategies}

The next agents become slightly more complex. These agents make use of the information given to them in the trick. The first one,
called HighLow, behaves as follows:
\begin{itemize}[noitemsep, label={}]
    \item if I have a card that can win the trick, play High
    \item otherwise, there's no hope, play Low
\end{itemize}
This HighLow strategy proves to perform quite well, and makes a lot of sense. If it is possible to take a trick, playing the best card
gives the highest chance of actually winning the trick. If there's no hope of winning a trick, playing your worst card
saves your better cards in the hopes that they can win later tricks.

A cooperative version of HighLow was created as well, called CoopHighLow. The logic of this agent is as follows:
\begin{itemize}[noitemsep, label={}]
    \item if partner has played in the trick and is winning, play Low
    \item otherwise, play HighLow
\end{itemize}
This strategy aims to not take tricks from your partner if it can be avoided. If your partner is winning the trick, saving your good cards for
later and allowing them to win will generally be a very powerful strategy. However, if your partner is losing the trick, or hasn't played,
the semi-aggressive HighLow strategy makes sense to play to try to win tricks or avoid losing better cards.


\subsection{Markov Decision Strategy}

The next agent uses a simplified Markov decision process to determine it's hand strength compared to the strength of remaining cards.
The agent uses the ratio of these values to guess at whether a card in his hand is strong and has a good chance of winning the trick.
At a high level, the Markov agent keeps track of all cards that have been seen so it knows what cards have not been seen. A value is
assigned to each card in the deck which represents the strength of that card. When it comes time to make a decision, the Markov agent
performs:
\begin{equation}
    \text{action}(H, D, \tau) = \left\{ \begin{array}{lr}
            \text{play High} & : \frac{Value(H)}{Value(U \setminus D)} \geq \tau \\
            \text{play Low}  & : \frac{Value(H)}{Value(U \setminus D)} < \tau 
        \end{array} \right. \nonumber
\end{equation}
where $H$ is the set of cards in the agent's hand, $D$ is the set of cards that have been seen and $U$ is the set of all cards. The function
$Value(\cdot)$ gives the sum of the values of all the cards in the given set. The threshold parameter $\tau$ determines the operating point
of the agent, as $\tau$ grows, the Markov agent becomes more and more aggressive. Essentially, when the Markov agent sees a high ratio value,
it seems it's hand as good, so a High card is played. Otherwise, the weak is viewed as weak and a Low card is played.

The specific values for $\tau$ and the cards used in our experiments are discussed in the next section.


\subsection{Card Counting Strategy}

The CardCounting agent tries to use all available information to it's advantage. This agent keeps track of whether or not each player can
possibly have a card or not. When a card is seen, it is known that no player can possibly hold that card in their hand, so this information
is tracked. For additional information, whenever a player does not follow the lead suit, it is known that the player cannot have any of the
lead suit in their hand. This information is also recorded.

In addition to using all available information, this agent employs a different strategy depending on how many cards are in the trick.
It tries to use players being out of a suit (or strong in a suit) to the team advantage. At a high level, the CardCounting agent performs
the following thought process before making a decision:

\begin{itemize}[label={}]
    \item \textbf{Leading the trick}
        \begin{itemize}[noitemsep]
            \item if partner is out of a suit and has trump, play Low in that suit hoping the partner can play trump
            \item if partner is ``strong'' in a suit compared to both other players, play Low in that suit
            \item otherwise, play High
        \end{itemize}
    \item \textbf{Second to play}
        \begin{itemize}[noitemsep]
            \item if partner can possibly win and is ``stronger'' than the other remaining player, play Low
            \item if partner doesn't have the lead suit:
                \begin{itemize}[noitemsep, label={}]
                    \item if trump was led, play HighLow
                    \item otherwise, if partner has trump, play Low
                \end{itemize}
            \item otherwise, play HighLow
        \end{itemize}
    \item \textbf{Third to play}
        \begin{itemize}[noitemsep]
            \item if partner is winning so far but the last player can win the trick, play HighLow
            \item otherwise, play CoopHighLow
        \end{itemize}
    \item \textbf{Last to play}
        \begin{itemize}[noitemsep]
            \item play CoopHighLow
        \end{itemize}
\end{itemize}

This method introduces complexity in the hopes of intelligent performance. This agent aims to maximize the number of tricks the team wins
through trying to let the partner take as many tricks as possible; except when the partner cannot help, where this agent tries to play selfishly
for the benefit of the team.

The word ``strong'' comes up in the above explanation. The strength of a player in a suit is defined as the sum of the ranks of
each card that player can possible have of that suit, where a high ranking means a stronger card. Then, $S_a > S_b$ implies that player $a$
has a stronger hand than player $b$ in that suit.

\subsection{Hybrid Strategy}

The Hybrid strategy combines the logics of the CoopHighLow, Markov and CardCounting agents. Essentially, this agent using the same Markov decision
process as the Markov agent, except when the agent's hand is good, the CoopHighLow strategy is played. Other times, the CardCounting strategy is played.
This strategy was created to hopefully compensate for the passive play of the CardCounting agent with the aggressive behaviour of the CoopHighLow agent.

\subsection{Monte Carlo Agent}

An overly simple strategy in euchre (or any card game) would be to simulate all possible games which follow from the current game state. In incomplete
information settings such as euchre, this is simply not computationally feasible. Instead of traversing the entire game tree, the MonteCarlo agent
randomly traverses the game tree for a set amount of time, keeping track of how many tricks were won in each path. This is similar to the strategy
laid out in \cite{skat}.

To help reduce the search space further, the MonteCarlo agent retains the idea from the CardCounting agent to keep track of which cards every
other player can possibly have. When it comes time to make a decision, a random possible hand is assigned to each other player and games are
simulated until finished. This process is repeated until some amount of time has passed, then the card that led to the most number of trick
wins is played. At the start of a game, this agent will likely perform poorly as the game tree is still enormous, while nearing the end of the
game, the game tree shrinks to a point where it is possible to simulate all possible games which will greatly strengthen play.

